{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>You can find the source code and contribute to the project on GitHub.</p> <p>Distributed Asynchronous k-mer Counting (DAKC)</p> <p>DAKC is the fastest distributed-memory parallel algorithm and software for k-mer counting. It is based on an asynchronous algorithm built on top of HCLib-Actor runtime system. It gives you upto \\(100\\times\\) speedup over commonly used k-mer counting tools like KMC3, and \\(2\\times\\) speedup over the previous fastest algorithm HySortK.</p> <p>Analytical Model of k-mer Counting</p> <p>First principle analytical model for distributed memory k-mer counting. Users can use this model to estimate the performance of k-mer counting on their target machine.</p> <p>PakMan* - A faster PakMan</p> <p>A faster version of the state of the art short-read genome assembly toolkit, PakMan. We replaced quicksort in PakMan with radix-sorting and tuned the performance, thereby speeding up its \\(k\\)-mer counting kernel by 2\\(\\times\\) across synthetic and real-world datasets.</p>"},{"location":"dakc/","title":"DAKC","text":"<p>DAKC is the fastest distributed-memory k-mer counting algorithm and software.  It is the only asynchronous k-mer counting tool that can perform k-mer counting without synchronizing the processors multiple times. </p> <p>DAKC GitHub Repository</p>"},{"location":"dakc/#prerequisite-hclib-actor-library","title":"Prerequisite: HCLib Actor Library","text":"<p>Follow the instructions at <code>https://hclib-actor.com</code> to download and install the HCLIB Actor runtime library.</p>"},{"location":"dakc/#input","title":"Input","text":"<p>Illumina paired-ended or single-ended <code>FASTQ</code> files.  For simplicity of reading the FASTQ files, we preprocess the data to remove the header file, making it simple to read the data in parallel using MPI I/O.  To avoid confusion, we store these preprocessed files as <code>.txt</code>.  The user can run the <code>fq2txtmaker.sh</code> script to generate the header removed input files from a given <code>FASTQ</code> file.</p>"},{"location":"dakc/#compile-time-variables-the-user-should-modify-based-on-their-use-case","title":"Compile time variables the user should modify based on their use case","text":"<ul> <li><code>KMERLEN</code>: The length \\(k\\) to use. Current implementation limits \\(k \\leq 32\\)</li> <li><code>READLEN</code>: Length of each read in the input <code>FASTQ</code> file.</li> <li><code>HITTER</code>: If <code>HITTER == 0</code>, then the \\(L_3\\) aggregation protocol is not performed, and vice versa.</li> <li><code>BIGKSIZE</code>: <code>2 x BIGKSIZE</code> is the \\(C_2\\) parameter size, mentioned in the paper.</li> <li><code>KCOUNT_BUCKET_SIZE</code>: The value of this parameter determines \\(C_3\\) parameter value. </li> <li><code>BENCHMARK</code>: If present, the program will generate statistics regarding the program's behavior and output.</li> </ul>"},{"location":"dakc/#how-to-compile","title":"How to compile","text":"<p>Open the Makefile and update <code>COMPILETIMEVARS</code> accordingly.  [Note: The user does not need to change anything for executing <code>DAKC</code> on synthetic datasets generated using data generation scripts provided to the user.]</p> <p>Type <code>make clean &amp;&amp; make</code> to compile DAKC.</p>"},{"location":"dakc/#how-to-execute","title":"How to execute","text":"<pre><code>srun -N &lt;num_nodes&gt; -n &lt;total_cores&gt; --cpu-bind=cores dakc -f &lt;input_file&gt;\n</code></pre> <p>Note: we recommend creating one process per physical core of the CPU for optimal performance.  In the above <code>srun</code> command, <code>&lt;total_cores&gt;</code> should be the total number of physical cores present in all the nodes being used for the execution.</p>"},{"location":"model/","title":"Analytical Model","text":"<p>The repository contains Python scripts for analytical model for distributed memory \\(k\\)-mer counting. </p> <p>The default scripts will reproduce plots from our conference paper, based on hardware parameters and experiments performed using the Phoenix supercomputer at Georgia Tech. </p> <p>DAKC GitHub Repository</p>"},{"location":"model/#description","title":"Description","text":"<ul> <li><code>kcount.py</code>: Funtion definitions for the analytical model of distributed-memory k-mer counting.</li> <li><code>memory.py</code>: Seperate model to show memory overhead of multi-layered message aggregation.</li> <li><code>params.py</code>: Machine parametes used for the analytical model. Edit this file with parameters of your target machine.</li> <li><code>experiments.py</code>: Input and experimental values of k-mer counting for different synthetic datasets, as observed on the Phoenix machine. Edit this file with updated input and experimental results on your target machine</li> <li><code>defaultplot.py</code>: Default plotting options.</li> <li><code>cachepred.py</code>: Analytically predicts the L3 cache misses for inputs mentioned in <code>experiments.py</code> and compares them against experimental results. </li> <li><code>hwresource.py</code>: Analytically predicts what percentage of k-mer counting is spent doing \"memory access\", \"communication\", and \"computation\". </li> </ul>"},{"location":"model/#how-to-execute","title":"How to execute","text":"<p>Edit the <code>params.py</code> and <code>experiments.py</code> file as required. Then simply run <code>python &lt;scriptname&gt;.py</code>. The script will produce a figure inside the <code>figures</code> directory.</p>"},{"location":"pkm/","title":"PakMan*: A Faster Version of PakMan","text":"<p>PakMan is a short-read genome assembly algorithm originally published by Ghosh et. al. at IPDPS 2019 conference.  In this repository, we have modified the original PakMan algorithm to use radix-sorting instead of quicksort.  This modification and some performance fine-tuning result in <code>PakMan*</code>, which has a \\(2\\times\\) faster \\(k\\)-mer counting kernel.</p> <p>PakMan* GitHub Repository</p> <p>Original PakMan GitHub Repository</p>"},{"location":"pkm/#how-to-run-pakman-vs-pakman","title":"How to Run PakMan vs PakMan*","text":"<p>If <code>-DFAST</code> flag is present during compilation, then <code>PakMan*</code> is executed. Otherwise, the executable is the same as the original <code>PakMan</code> algorithm.</p>"},{"location":"pkm/#how-to-compile-pakman","title":"How to Compile PakMan*","text":"<p>Type <code>make clean &amp;&amp; make</code> to compile PakMan* </p>"},{"location":"pkm/#how-to-execute-pakman","title":"How to execute PakMan*","text":"<p>Below are the instructions for running PakMan* on a synthetic dataset generated using our scripts.</p> <pre><code>srun -N &lt;num_nodes&gt; -n &lt;total_cores&gt; --cpu-bind=cores pakman -r 150 -c 50 -b 1000000000 -t 21 -n 100000 -f &lt;input_fasta_file&gt;\n</code></pre>"},{"location":"pkm/#_1","title":"PakMan*","text":"<p>Below is an excerpt from the README file of the original PakMan repository, which serves as an excellent resource for understanding the usage of the PakMan toolkit.  For more info please visit the original github repository of PakMan.</p>"},{"location":"pkm/#pakman-a-scalable-algorithm-for-generating-genomic-contigs-on-distributed-memory-machines","title":"PaKman: A Scalable Algorithm for Generating Genomic Contigs on Distributed Memory Machines","text":"<p>We address the problem of performing large-scale genome assemblies on a distributed memory parallel computer. PaKman presents a solution for the two most time-consuming phases in the full genome assembly pipeline, namely, k-mer Counting and Contig Generation. A key aspect of our algorithm is its graph data structure (PaK-Graph), which comprises fat nodes (or what we call \"macro-nodes\") that reduce the communication burden during contig generation.</p>"},{"location":"pkm/#dependencies","title":"Dependencies:","text":"<p>PaKman has the following dependencies: * MPI library (preferably MPI-3 compatible) * C++14 (or greater) compliant compiler</p> <p>Note:  At this time, PaKman requires the input files to be in the FASTA format and utilizes single-end reads to generate the contigs. We are working to extend the functionality to  perform scaffolding accepting paired-end reads as input. </p>"},{"location":"pkm/#build","title":"Build:","text":"<p>Please specify the k-mer length at the time of building:</p> <p>For e.g: <code>make ksize=32</code></p> <p>If ksize is not specified at the time of build, PaKman will build with default size of k=31.</p> <p>Note: * At present, PaKman supports k-mer lengths of k&lt;=64. * Pass/update -DLMER_LENGTH in the Makefile to update the LMER size. Default value is set to 8 (recommended).</p>"},{"location":"pkm/#execute","title":"Execute:","text":"<p><code>mpiexec -np $procs $BINARY -f $INPUT -b $MAX_BATCH_SIZE -r $AVG_READ_LEN -c $COVERAGE -t $BUCKET_CUTOFF -n $MERGE_CUTOFF</code></p> <p>For example:</p> <p><code>mpiexec -np 4 ./pkmer -f ~/string-graph/inputs/Ecoli_reads_80x.fasta -b 100000000 -r 100 -c 80 -t 21 -n 100000</code></p>"},{"location":"pkm/#mandatory-input-arguments-to-pakman","title":"Mandatory input arguments to PaKman:","text":"<ol> <li><code>-f &lt;INPUT&gt;</code>: input reads file in .fasta format</li> <li><code>-b &lt;MAX_BATCH_SIZE&gt;</code>: number of k-mers to include in a batch for collective communication. Default value set to 100M (100,000,000). If memory is short, consider reducing to 50M.</li> <li><code>-r &lt;AVG_READ_LENGTH&gt;</code>: average length of the short reads.</li> <li><code>-c &lt;COVERAGE&gt;</code>: coverage of the input reads dataset</li> <li><code>-t &lt;BUCKET_CUTOFF&gt;</code>: number of buckets to consider while determining the cutoff from the k-mer frequency histogram. Default value set to 21.</li> <li><code>-n &lt;MERGE_CUTOFF&gt;</code>: number of nodes at which the iterative phase of merging macro-nodes is concluded, Default value set to 100,000.</li> </ol>"},{"location":"pkm/#publications","title":"Publications:","text":"<p>1) Ghosh, Priyanka, Sriram Krishnamoorthy, and Ananth Kalyanaraman. \"PaKman: A Scalable Algorithm for Generating Genomic Contigs on Distributed Memory Machines.\" IEEE Transactions on Parallel and Distributed Systems (TPDS) vol. 32, no. 5, pp. 1191-1209, 2021. DOI: 10.1109/TPDS.2020.3043241. </p> <p>2) Ghosh, Priyanka, Sriram Krishnamoorthy, and Ananth Kalyanaraman. \"PaKman: Scalable Assembly of Large Genomes on Distributed Memory Machines.\" In 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS), pp. 578-589. IEEE, 2019.</p>"},{"location":"pkm/#license","title":"License:","text":"<p>This project is licensed under the BSD License, see LICENSE file for details.</p>"},{"location":"pkm/#acknowledgments","title":"Acknowledgments:","text":"<p>This research used resources of the NERSC Office of Science User Facility supported by U.S. DOE under Contract No. DE-AC02-05CH11231.  This work was supported in part by U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research  under award number 63823. Pacific Northwest National Laboratory is operated by Battelle for DOE under Contract DE-AC05-76RL01830.  The research is in parts supported by U.S. NSF grants CCF 1815467, OAC 1910213, and CCF 1919122 to Washington State University.</p>"}]}